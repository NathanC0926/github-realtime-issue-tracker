# Integrating Text Numeric  
## DS 340 Final Project Proposal  

### 1. Team  
- Ze Song  
- Chi Hin Nathan Chang  

### 2. Elevator Pitch  
**Predict GitHub issue resolution times using hybrid NLP and XGBoost models.**

### 3. Context  
The dataset integrates textual data (issue titles and descriptions) and structured/tabular data (timestamps, labels, assignees).

**Key Data Fields:**  
- **Issue Metadata:** `issue_title`, `issue_body`, `labels`, `milestone`  
- **Event Information:** `action` (created, closed, reopened)  
- **User & Engagement:** `issue_creator` (bot indicator), `author_association` (OWNER, MEMBER, etc.), `num_assignees`  
- **Timestamps:** created, updated, closed  

### 4. Methods  

**Baseline 1: TF-IDF + LightGBM/XGBoost**  
- Convert issue text to TF-IDF vectors.  
- Concatenate with numeric features (comments, labels, timestamps).  
- *Reason:* Quick setup, computationally efficient.  

**Baseline 2: CatBoost with Native Text Handling**  
- Feed raw issue text directly.  
- Automatic text processing.  
- *Reason:* Simplifies setup, leverages built-in handling.  

**Iteration 1: Pretrained Embeddings (Word2Vec/GloVe) + LightGBM/XGBoost**  
- Convert issue text to dense word embeddings.
- *Reason:* Captures semantic relationships, lower dimensionality than TF-IDF, computationally efficient. 

**Iteration 2: BERT Embeddings + Gradient Boosting**  
- Extract `[CLS]` token embeddings from BERT (768-dim).  
- Feed embeddings with numeric features into LightGBM/XGBoost.  
- *Reason:* Enhanced contextual representation.  

**Iteration 3: TabNet (Optional)**  
- Concatenate text embeddings with numeric features.  
- TabNet dynamically learns feature importance.  
- *Reason:* Effective feature interaction and interpretability.  

**Iteration 4: BERT + MLP Hybrid (Optional)**  
- Combine BERT embeddings with numeric data.  
- Dual-branch architecture: BERT and MLP models merged at the final layer.  
- *Reason:* High accuracy, higher computational cost.  

**Additional Considerations:**  
- **Missing Data:** Impute (mean/mode) or native handling  
- **Feature Engineering:** Issue length, weekday opened, label presence  
- **Scaling:** Standardize numeric inputs for deep models  

**Methodology Summary:**  
| Iteration                  | Text Processing                  | Model                                 | Reason                                     |
|----------------------------|----------------------------------|--------------------------------------- |-------------------------------------|
| Baseline 1                 | TF-IDF vectors                   | LightGBM/XGBoost                      | Quick setup, efficient               |
| Baseline 2                 | CatBoost Native                   | CatBoost                              | Native handling, simplified setup    |
| Iteration 1                | Pretrained Embeddings (Word2Vec/GloVe)                           | XGBoost/LightGBM                      | Fairly Fast, efficient                     |
| Iteration 2                | BERT Embeddings ([CLS])          | XGBoost/LightGBM                      | Contextual understanding            |
| Iteration 3 (Optional)     | Embeddings + Tabular             | TabNet                                | Adaptive feature selection           |
| Iteration 4 (Optional)     | BERT fine-tuned embeddings       | BERT + MLP (Deep learning, merged)    | Highest accuracy (higher cost)      |

---

# 5. Data Source

### **GitHub Archive Dataset**  
ðŸ“Œ **Source**: [GitHub Archive](https://www.gharchive.org/)

ðŸ“Œ **If using GCP**: [BigQuery](https://console.cloud.google.com/bigquery?project=githubarchive&page=project/)

We will extract data and clean it ourselves.

### 6. Code Resources  
- **ML Libraries:** `scikit-learn`, `XGBoost`, `LightGBM`, `CatBoost`, `PyTorch`, `Hugging Face`, etc 
- **Data Processing:** `pandas`, `NumPy`, `SQL`  
- **Computing:** `Google Colab` Or `BUSCC`

### 7. Novel Contributions  
- Integration of textual data with tabular GitHub issue data.  
- Hybrid model combining NLP embeddings (BERT) and XGBoost/LightGBM.  
- Custom feature engineering to enhance predictive power.   

### 8. Timeline & Deliverables  
| Milestone         | Deliverable                                            | Due Date        |
|-------------------|--------------------------------------------------------|------------------|
| Milestone 1       | Data extraction and preprocessing                      | March 25           |
| Milestone 2       | Baseline models trained (TF-IDF, CatBoost)             | Apr 1                               |
| Milestone 3       | Pretrained Embeddings (Word2Vec/GloVe) + XGBoost/LightGBM                               | Apr 5                               |
| Milestone 4       | BERT embeddings + XGBoost/LightGBM                      | Apr 10                               |
| Milestone 5       | TabNet experimentation                                  | Apr 15                               |
| Milestone 6       | BERT experimentation                                  | Apr 20                               |
| Final Submission  | Final model selection & report submission               | Apr 30                               |

### 9. Demonstration & Evaluation  
- **Evaluation Metrics:** Mean Absolute Error (MAE)  
- **Baseline Comparison:** Evaluate against simple regression models. 

## 10. Experiments
ðŸ”¬ **Mentioned at #4 in detail**   

---
