# Integrating Text Numeric  
## DS 340 Final Project Proposal  

### 1. Team  
- Ze Song  
- Chi Hin Nathan Chang  

### 2. Elevator Pitch  
**Predict GitHub issue resolution times using hybrid NLP and XGBoost models.**

### 3. Context  
The dataset integrates textual data (issue titles and descriptions) and structured/tabular data (timestamps, labels, assignees).

**Key Data Fields:**  
- **Issue Metadata:** `issue_title`, `issue_body`, `labels`, `milestone`  
- **Event Information:** `action` (created, closed, reopened)  
- **User & Engagement:** `issue_creator` (bot indicator), `author_association` (OWNER, MEMBER, etc.), `num_assignees`  
- **Timestamps:** created, updated, closed  

### 4. Methods  

**Baseline 1: TF-IDF + LightGBM/XGBoost**  
- Convert issue text to TF-IDF vectors.  
- Concatenate with numeric features (comments, labels, timestamps).  
- *Reason:* Quick setup, computationally efficient.  

**Baseline 2: CatBoost with Native Text Handling**  
- Feed raw issue text directly.  
- Automatic text processing.  
- *Reason:* Simplifies setup, leverages built-in handling.  

**Iteration 1: Pretrained Embeddings (Word2Vec/GloVe) + LightGBM/XGBoost**  
- Convert issue text to dense word embeddings.
- *Reason:* Captures semantic relationships, lower dimensionality than TF-IDF, computationally efficient. 

**Iteration 2: BERT Embeddings + Gradient Boosting**  
- Extract `[CLS]` token embeddings from BERT (768-dim).  
- Feed embeddings with numeric features into LightGBM/XGBoost.  
- *Reason:* Enhanced contextual representation.  

**Iteration 3: TabNet (Optional)**  
- Concatenate text embeddings with numeric features.  
- TabNet dynamically learns feature importance.  
- *Reason:* Effective feature interaction and interpretability.  

**Iteration 4: BERT + MLP Hybrid (Optional)**  
- Combine BERT embeddings with numeric data.  
- Dual-branch architecture: BERT and MLP models merged at the final layer.  
- *Reason:* High accuracy, higher computational cost.  

**Additional Considerations:**  
- **Missing Data:** Impute (mean/mode) or native handling  
- **Feature Engineering:** Issue length, weekday opened, label presence  
- **Scaling:** Standardize numeric inputs for deep models  

**Methodology Summary:**  
| Iteration                  | Text Processing                  | Model                                 | Reason                                     |
|----------------------------|----------------------------------|--------------------------------------- |-------------------------------------|
| Baseline 1                 | TF-IDF vectors                   | LightGBM/XGBoost                      | Quick setup, efficient               |
| Baseline 2                 | CatBoost Native                   | CatBoost                              | Native handling, simplified setup    |
| Iteration 1                | Pretrained Embeddings (Word2Vec/GloVe)                           | XGBoost/LightGBM                      | Fairly Fast, efficient                     |
| Iteration 2                | BERT Embeddings ([CLS])          | XGBoost/LightGBM                      | Contextual understanding            |
| Iteration 3 (Optional)     | Embeddings + Tabular             | TabNet                                | Adaptive feature selection           |
| Iteration 4 (Optional)     | BERT fine-tuned embeddings       | BERT + MLP (Deep learning, merged)    | Highest accuracy (higher cost)      |

---

# 5. Data Source

### **GitHub Archive Dataset**  
ðŸ“Œ **Source**: [GitHub Archive](https://www.gharchive.org/)

ðŸ“Œ **If using GCP**: [BigQuery](https://console.cloud.google.com/bigquery?project=githubarchive&page=project/)

We will extract data and clean it ourselves.

### 6. Code Resources  
- **ML Libraries:** `scikit-learn`, `XGBoost`, `LightGBM`, `CatBoost`, `PyTorch`, `Hugging Face`, etc 
- **Data Processing:** `pandas`, `NumPy`, `SQL`  
- **Computing:** `Google Colab` Or `BUSCC`

### 7. Novel Contributions  
- Integration of textual data with tabular GitHub issue data.  
- Hybrid model combining NLP embeddings (BERT) and XGBoost/LightGBM.  
- Custom feature engineering to enhance predictive power.   

### 8. Timeline & Deliverables  
| Milestone         | Deliverable                                            | Due Date        |
|-------------------|--------------------------------------------------------|------------------|
| Milestone 1       | Data extraction and preprocessing                      | March 25           |
| Milestone 2       | Baseline models trained (TF-IDF, CatBoost)             | Apr 1                               |
| Milestone 3       | Pretrained Embeddings (Word2Vec/GloVe) + XGBoost/LightGBM                               | Apr 5                               |
| Milestone 4       | BERT embeddings + XGBoost/LightGBM                      | Apr 10                               |
| Milestone 5       | TabNet experimentation                                  | Apr 15                               |
| Milestone 6       | BERT experimentation                                  | Apr 20                               |
| Final Submission  | Final model selection & report submission               | Apr 30                               |

### 9. Demonstration & Evaluation  
- **Evaluation Metrics:** Mean Absolute Error (MAE)  
- **Baseline Comparison:** Evaluate against simple regression models. 

### 10. Experiments
#### To comprehensively evaluate our GitHub issue resolution prediction model, we can conduct the following experiments:

- Experiment 1: Comparison of Embedding Techniques

Evaluate the model performance using different text embeddings including TF-IDF, Word2Vec, GloVe, and BERT embeddings. Each embedding technique will be tested with XGBoost  and LightGBM to determine the most effective combination.
Metrics: Mean Absolute Error (MAE) will be used to quantify differences.

- Experiment 2: Hyperparameter Tuning

Conduct hyperparameter optimization for gradient boosting methods (XGBoost and LightGBM). Parameters such as depth, learning rate, number of estimators, and regularization will be systematically varied and assessed.
Evaluation: Identify the best parameter set based on validation MAE scores.

- Experiment 3: Impact of Feature Engineering

Measure the performance impact of custom feature engineering, including issue length, day of the week issue creation, label presence, and assignee count. Evaluate model performance with and without these engineered features.
Metrics: Changes in MAE will be analyzed to assess the feature engineering contribution.

- Experiment 4: Model Robustness and Generalization

Evaluate how well the final model generalizes to unseen repositories. Split data by repository or by period (time-based splitting) to test robustness.
Evaluation Criteria: Compare MAE across different splits to assess generalization capability.

These experiments are designed to ensure a robust evaluation of the proposed hybrid NLP and gradient boosting models, providing insights into optimal configurations and model behavior under varying conditions.

---
