# DS 340 Final Project Proposal

## 1. Team
- **Ze Song**  
- **Chi Hin Nathan Chang**  

(*Each teammate will submit the same proposal.*)  

## 2. Elevator Pitch
We aim to train a machine learning model to predict how long a GitHub issue will remain open.  

## 3. Context
GitHub is the leading platform for developers to collaborate and manage open-source and private projects. A key aspect of project management in GitHub is issue tracking, where developers report bugs, suggest feature requests, and document tasks. However, predicting how long an issue will remain open is challenging due to factors like issue complexity, contributor engagement, and repository activity.  

Accurately estimating issue resolution times can:  
- Improve project planning and prioritization.  
- Help maintainers allocate resources efficiently.  
- Reduce bottlenecks in software releases.  
- Enhance contributor experience and retention.  

For large open-source projects, knowing which issues will likely take longer to resolve enables better decision-making. Our project aims to provide a data-driven approach to predicting issue resolution time using structured data (e.g., labels, timestamps) and unstructured text (e.g., issue descriptions, comments).  

---

## 4. Methods

### 1. Text Feature Extraction (NLP Component)
Since issue titles and descriptions are critical in understanding issue resolution time, we will convert unstructured text into numerical features using:  

#### **Lightweight Approaches (For Tree-Based Models like XGBoost)**
- **TF-IDF + SVD/PCA**: Useful for extracting high-impact words from short text like issue titles.  
- **Word Embeddings (Word2Vec, FastText, GloVe)**: Averaging embeddings per issue description provides meaningful vector representations of textual data.  

#### **Advanced Approaches (For Hybrid Models & Deep Learning)**
- **Transformer-Based Models (BERT, RoBERTa, DistilBERT, T5)**: Extract contextual embeddings to capture issue semantics.  
- **Sentence Transformers (SBERT, MiniLM)**: Efficient text-to-vector conversion, preserving sentence meaning.  

### 2. Final Prediction Models (Tabular + Text Fusion)
Our model will integrate structured tabular data (categorical, numerical, etc.) with text embeddings from NLP models. The best approaches include:  

#### **Tree-Based Models (Best for Efficiency & Interpretability)**
- **XGBoost** *(Recommended Model)*
  - Well-suited for structured data + text embeddings.  
  - Handles missing values and categorical data efficiently.  
- **LightGBM / CatBoost**
  - LightGBM is faster for large datasets.  
  - CatBoost is optimized for categorical features.  

#### **High difficulty: Deep Learning-Based Models (For End-to-End Learning)**
- **TabNet**: Attention-based tabular model that integrates text embeddings directly.  
- **Transformer-Based Fusion (BERT + MLP)**:  
  - BERT handles text features.  
  - MLP processes structured data.  
  - Requires significant computing power.  
- **Multimodal Transformers (TABBIE, FT-Transformer)**:  
  - Designed specifically for text + tabular fusion.  

---

## 5. Data Source

### **GitHub Archive Dataset**  
ðŸ“Œ **Source**: [GitHub Archive](https://www.gharchive.org/)  
ðŸ“Œ **Format**: Compressed JSON  
ðŸ“Œ **Update Frequency**: Hourly  

GitHub Archive provides structured event logs for all public GitHub activity, making it a rich source for analyzing issue resolution patterns.  

### **Relevant Events for Our Prediction Task:**  
1. **IssuesEvent**  
   - Issue creation (`"action": "opened"`)  
   - Issue closure (`"action": "closed"`)  
   - Timestamps for computing resolution time  
2. **IssueCommentEvent**  
   - Captures discussions around issues  
   - Helps understand contributor engagement  

By aggregating these events, we can extract:  
- **Issue metadata**: Labels, assigned contributors, timestamps  
- **Issue description & comments**: NLP features  
- **Repository metadata**: Star count, forks, contributor activity  

---

## 6. Code Resources
We will use:  
- **ML Libraries**: scikit-learn, XGBoost, LightGBM, CatBoost, PyTorch, Hugging Face Transformers  
- **Data Processing**: pandas, NumPy, SQL  
- **Cloud & Computing**: Google Colab / AWS (if needed for larger models)  

---

## 7. Whatâ€™s New?
Our novel contributions include:  
 A **custom dataset** integrating structured and unstructured GitHub data.  
 **Hybrid model experiments** combining tree-based models with transformer embeddings.  
 **New feature engineering techniques** for issue resolution patterns.  
 **Evaluation of real-world feasibility** for open-source project maintainers.  

---

## 8. Plan & Milestones

ðŸ—“ **Milestone 1 (March 31)**: Data collection & preprocessing complete  
   - Extract GitHub issues & comments from GitHub Archive  
   - Clean and preprocess text data  
   - Engineer relevant features for structured data  

ðŸ—“ **Milestone 2 (April 15)**: First model trained & evaluated  
   - Train baseline XGBoost model  
   - Experiment with text feature extraction methods  
   - Evaluate initial model performance  

ðŸ—“ **Final Submission (April 30)**: Best model selected & final report completed  

---

## 9. Proposed Demonstration or Evaluation

ðŸ“Š **Success Metrics**:  
- Mean Absolute Error (MAE) for regression models.  
- Accuracy / F1-score if we use classification (binning issue resolution times into categories like â€œ<1 day,â€ â€œ1 week,â€ etc.).  
- Comparison to baseline methods (e.g., simple heuristics like median issue resolution time).  

---

## 10. Experiments

ðŸ”¬ **Major Experiment 1: Varying Text Representations**  
- TF-IDF vs. Word2Vec vs. BERT-based embeddings  
- Evaluating which method best captures issue complexity  

ðŸ”¬ **Major Experiment 2: Model Comparisons**  
- XGBoost vs. LightGBM vs. TabNet vs. Transformer-based models  
- Analyzing trade-offs in accuracy, interpretability, and training time  

---

