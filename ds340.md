# DS 340 Final Project Proposal

## 1. Team
- **Ze Song**  
- **Chi Hin Nathan Chang**  

## 2. Elevator Pitch
Train an XGBoost and NLP model to predict how long a GitHub issue will remain open. chi hin 

## 3. Context
### Structured & Unstructured Data  
The dataset contains both **textual fields** (e.g., issue title, description) and **tabular fields** (e.g., timestamps, labels, assignees, comment count).  

### Key Data Fields  

#### **Issue Metadata**  
- **issue_title, issue_body**: The title and description of the issue (key text features).  
- **labels**: A list of assigned tags, useful for categorization.  
- **milestone**: Information about project milestones linked to the issue.  

#### **Event Information**  
- **action**: The type of event (`created`, `closed`, `reopened`).  

#### **User & Engagement Features**  
- **issue_creator**: Struct containing creator details, including whether they are a bot.  
- **author_association**: The role of the issue creator (`OWNER`, `MEMBER`, `CONTRIBUTOR`, etc.).  
- **num_assignees**: The number of people assigned to the issue.  

#### **Timestamps for Predicting Resolution Time**  
- **issue_created_at**: The timestamp when the issue was first created.  
- **issue_closed_at**: The timestamp when the issue was closed (used to compute resolution time).  
---

## 4. Methods

### Baseline 1: TF-IDF + LightGBM/XGBoost
- **Text Processing:** Convert issue descriptions to TF-IDF vectors.
- **Model:** Use LightGBM or XGBoost with concatenated text and numeric features.
- **Feature Engineering:** Include metadata (e.g., number of comments, labels, timestamps).
- **Why:** Fast and effective, works well on structured data, minimal compute requirements.

### Baseline 2: CatBoost (with Native Text Handling)
- **Text Processing:** Feed raw text directly into CatBoost.
- **Model:** CatBoost automatically generates text-based features (e.g., TF-IDF, BM25).
- **Why:** Simplifies preprocessing, improves handling of categorical and text features.

### Iteration 1: Pretrained Embeddings (Word2Vec/GloVe) + Gradient Boosting
- **Text Processing:** Convert issue text to dense word embeddings (e.g., Word2Vec or GloVe, averaged across words).
- **Model:** Concatenate dense text embeddings with structured numeric data in LightGBM/XGBoost.
- **Why:** Captures semantic relationships, lower dimensionality than TF-IDF, computationally efficient.

### Iteration 2: BERT Embeddings + Gradient Boosting
- **Text Processing:** Extract fixed-size embeddings from a pretrained BERT model (e.g., [CLS] token vector, 768-dim).
- **Model:** Feed extracted BERT embeddings into LightGBM/XGBoost.
- **Why:** Context-aware text representation, but still computationally feasible without full fine-tuning.

### Iteration 3: TabNet for Joint Text-Tabular Modeling (If Time and Resources Allow)
- **Text Processing:** Concatenate text features (from TF-IDF/embeddings) with structured data.
- **Model:** Use TabNet, which learns feature importance dynamically.
- **Why:** Handles mixed data types well, interpretable, more effective for complex relationships.

### Final Iteration: Multi-Input Neural Network (BERT + MLP) (If Time and Resources Allow)
- **Text Processing:** Fine-tune a BERT model on issue descriptions.
- **Numeric Processing:** Feed structured numeric features through a Multi-Layer Perceptron (MLP).
- **Model:** Two possible approaches:
  - **Token-Separated Concatenation:** Convert numeric/categorical features into text format (e.g., `"Priority: High [SEP] Created_on_Friday: Yes"`) and prepend them to the issue description before feeding into BERT.
  - **Multi-Input Model:** Use a dual-branch architecture, where BERT processes text, and a separate MLP processes structured features; merge outputs before final classification.
- **Why:** Best possible performance by leveraging full-text understanding and structured data interactions, but highest computational cost.

### Additional Considerations:
- **Missing Data:** Use LightGBM/XGBoost’s native handling or impute missing values (mean/mode for numeric, special token for text).
- **Feature Engineering:** Add features like issue length, label presence, weekday opened.
- **Scaling:** Standardize numeric inputs for deep learning models, but not required for tree-based models.

### Summary:
1. **Start Simple:** TF-IDF + LightGBM/XGBoost (fast and effective baseline).
2. **Improve Efficiency:** CatBoost’s native text handling for automation.
3. **Add Semantics:** Word2Vec/GloVe embeddings for better text representation.
4. **Use Deep Features:** BERT embeddings enhance textual understanding.
5. **Tabular-Aware Models:** TabNet can learn feature importance dynamically.
6. **Final Model:** BERT fine-tuning + MLP fusion for maximum accuracy if resources allow.

Each step progressively improves accuracy while considering computational feasibility.

---

## 5. Data Source

### **GitHub Archive Dataset**  
📌 **Source**: [GitHub Archive](https://www.gharchive.org/)

📌 **If using GCP**: [BigQuery](https://console.cloud.google.com/bigquery?project=githubarchive&page=project/)

We will extract data and clean it ourselves.

---

## 6. Code Resources
We will use:  
- **ML Libraries**: scikit-learn, XGBoost, LightGBM, CatBoost, PyTorch, Hugging Face Transformers  
- **Data Processing**: pandas, NumPy, SQL  
- **Cloud & Computing**: Google Colab (if needed)  

---

## 7. What’s New?
Our novel contributions include:  
 A **custom dataset** integrating structured and unstructured GitHub data.  
 **Hybrid model experiments** combining tree-based models with transformer embeddings.  
 **New feature engineering techniques** for issue resolution patterns.  
 **Evaluation of real-world feasibility** for open-source project maintainers.  

---

## 8. Plan & Milestones

🗓 **Milestone 1 (March 25)**: Data collection & preprocessing complete  
   - Extract GitHub issues & comments from GitHub Archive  
   - Clean and preprocess text data  
   - Engineer relevant features for structured data  

🗓 **Milestone 2 (April 1)**: Baseline model trained & evaluated   
   - Experiment with text feature extraction methods  
   - Evaluate initial model performance
     
🗓 **Milestone 3 (April 10)**: Iteration 1-2

🗓 **Milestone 4 (April 20)**: Iteration 3-4  

🗓 **Final Submission (April 30)**: Best model selected & final report completed  

---

## 9. Proposed Demonstration or Evaluation

📊 **Success Metrics**:  
- Mean Absolute Error (MAE) for regression models.  
- Macro F1-score if we use multi-class classification (binning issue resolution times into categories like “<1 day,” “1 week,” etc.).  
- Comparison to baseline methods (e.g., simple heuristics like median issue resolution time).  

---

## 10. Experiments
🔬 **Mostly mentioned at #4**   

---

