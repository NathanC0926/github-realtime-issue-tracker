# DS 340 Final Project Proposal

## 1. Team
- **Ze Song**  
- **Chi Hin Nathan Chang**  

## 2. Elevator Pitch
Train an XGBoost and NLP model to predict how long a GitHub issue will remain open. chi hin 

## 3. Context
### Structured & Unstructured Data  
The dataset contains both **textual fields** (e.g., issue title, description) and **tabular fields** (e.g., timestamps, labels, assignees, comment count).  

### Key Data Fields  

#### **Issue Metadata**  
- **issue_title, issue_body**: The title and description of the issue (key text features).  
- **labels**: A list of assigned tags, useful for categorization.  
- **milestone**: Information about project milestones linked to the issue.  

#### **Event Information**  
- **action**: The type of event (`created`, `closed`, `reopened`).  

#### **User & Engagement Features**  
- **issue_creator**: Struct containing creator details, including whether they are a bot.  
- **author_association**: The role of the issue creator (`OWNER`, `MEMBER`, `CONTRIBUTOR`, etc.).  
- **num_assignees**: The number of people assigned to the issue.  

#### **Timestamps for Predicting Resolution Time**  
- **issue_created_at**: The timestamp when the issue was first created.  
- **issue_closed_at**: The timestamp when the issue was closed (used to compute resolution time).  
---

## 4. Methods

### 1. Text Feature Extraction (NLP Component)
Since issue titles and descriptions are critical in understanding issue resolution time, we will convert unstructured text into numerical features using:  

#### **Lightweight Approaches (For Tree-Based Models like XGBoost)**
- **TF-IDF + SVD/PCA**: Useful for extracting high-impact words from short text-like issue titles.  
- **Word Embeddings (Word2Vec, FastText, GloVe)**: Averaging embeddings per issue description provides meaningful vector representations of textual data.  

#### **Advanced Approaches (For Hybrid Models & Deep Learning)**
- **Transformer-Based Models (BERT, RoBERTa, DistilBERT, T5)**: Extract contextual embeddings to capture issue semantics.  
- **Sentence Transformers (SBERT, MiniLM)**: Efficient text-to-vector conversion, preserving sentence meaning.  

### 2. Final Prediction Models (Tabular + Text Fusion)
Our model will integrate structured tabular data (categorical, numerical, etc.) with text embeddings from NLP models. The best approaches include:  

#### **Tree-Based Models (Best for Efficiency & Interpretability)**
- **XGBoost** *(Recommended Model)*
  - Well-suited for structured data + text embeddings.  
  - Handles missing values and categorical data efficiently.  
- **LightGBM / CatBoost**
  - LightGBM is faster for large datasets.  
  - CatBoost is optimized for categorical features.  

#### **High difficulty: Deep Learning-Based Models (For End-to-End Learning)**
- **TabNet**: Attention-based tabular model that integrates text embeddings directly.  
- **Transformer-Based Fusion (BERT + MLP)**:  
  - BERT handles text features.  
  - MLP processes structured data.  
  - Requires significant computing power.  
- **Multimodal Transformers (TABBIE, FT-Transformer)**:  
  - Designed specifically for text + tabular fusion.  

---

## 5. Data Source

### **GitHub Archive Dataset**  
ðŸ“Œ **Source**: [GitHub Archive](https://www.gharchive.org/)  [BigQuery]https://console.cloud.google.com/bigquery?project=githubarchive&page=project

We will extract data and clean it ourselves.

---

## 6. Code Resources
We will use:  
- **ML Libraries**: scikit-learn, XGBoost, LightGBM, CatBoost, PyTorch, Hugging Face Transformers  
- **Data Processing**: pandas, NumPy, SQL  
- **Cloud & Computing**: Google Colab / AWS (if needed for larger models)  

---

## 7. Whatâ€™s New?
Our novel contributions include:  
 A **custom dataset** integrating structured and unstructured GitHub data.  
 **Hybrid model experiments** combining tree-based models with transformer embeddings.  
 **New feature engineering techniques** for issue resolution patterns.  
 **Evaluation of real-world feasibility** for open-source project maintainers.  

---

## 8. Plan & Milestones

ðŸ—“ **Milestone 1 (March 31)**: Data collection & preprocessing complete  
   - Extract GitHub issues & comments from GitHub Archive  
   - Clean and preprocess text data  
   - Engineer relevant features for structured data  

ðŸ—“ **Milestone 2 (April 15)**: First model trained & evaluated  
   - Train baseline XGBoost model  
   - Experiment with text feature extraction methods  
   - Evaluate initial model performance  

ðŸ—“ **Final Submission (April 30)**: Best model selected & final report completed  

---

## 9. Proposed Demonstration or Evaluation

ðŸ“Š **Success Metrics**:  
- Mean Absolute Error (MAE) for regression models.  
- Macro F1-score if we use multi-class classification (binning issue resolution times into categories like â€œ<1 day,â€ â€œ1 week,â€ etc.).  
- Comparison to baseline methods (e.g., simple heuristics like median issue resolution time).  

---

## 10. Experiments

ðŸ”¬ **Major Experiment 1: Varying Text Representations**  
- TF-IDF vs. Word2Vec vs. BERT-based embeddings  
- Evaluating which method best captures issue complexity  

ðŸ”¬ **Major Experiment 2: Model Comparisons**  
- XGBoost vs. LightGBM vs. TabNet vs. Transformer-based models  
- Analyzing trade-offs in accuracy, interpretability, and training time  

---

